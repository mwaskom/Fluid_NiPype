#! /usr/bin/env python
import os
from os.path import join as pjoin
import argparse
import numpy as np

import nipype.interfaces.fsl as fsl
import nipype.interfaces.freesurfer as fs
import nipype.interfaces.io as nio
import nipype.interfaces.utility as util
import nipype.pipeline.engine as pe

from fluid_utility import archive_crashdumps

# Parse command line arguments
parser = argparse.ArgumentParser(description="Nipype script to run augmented-TBSS analysis.")

parser.add_argument("-subjects", nargs="*",
                    metavar="subject_id",
                    help="process subject(s)")
parser.add_argument("-workflows", nargs="*", metavar="<wf>", choices=["reg","skel","all"],
                    help="which workflows to run")
parser.add_argument("-groupfile", metavar="<file>",
                    help="text file of subject ids to include in group analysis")
parser.add_argument("-clgroup", action="store_true",
                    help="command-line subject list defines group")
parser.add_argument("-r1", action="store_true",
                    help="processes and project R1 values")
parser.add_argument("-ipython",action="store_true")
parser.add_argument("-multiproc",action="store_true")
args = parser.parse_args()

# Set up some paths
project_dir = "/mindhive/gablab/fluid"
data_dir = pjoin(project_dir, "Data")
nipype_dir = pjoin(project_dir, "Analysis/Nipype")
analysis_dir = pjoin(nipype_dir, "tbss")
working_dir = pjoin(nipype_dir, "workingdir", "tbss")

if not os.path.exists(analysis_dir):
    os.makedirs(analysis_dir)

if os.path.isfile(args.subjects[0]):
    subjects = np.loadtxt(args.subjects[0],str).tolist()
else:
    subjects = args.subjects

# Set up a node to supply subject ids
subjsource = pe.Node(util.IdentityInterface(fields=["subject_id"]),
                     iterables=("subject_id", subjects),
                     name="subjectsource")

#---------------------------------------------------------------------#
# Registration
#---------------------------------------------------------------------#

# Grab the FA B=0, and radialdiff DWI images 
# (for us, these were generated by dt_recon)
# and the Freesurfer Anat -> MNI warpfield
# Optionally (based on command-line) grab the T1.mgz images
# Note that this is NOT the recon-all T1.mgz, but is actually
# a map of T1 relaxation time. Generated from our FLASH scans.
outfields=["fa_image", "lowb_image", "radial_diff", "warpfield"]
if args.r1:
    outfields.append("t1_image")
reggrabber = pe.Node(nio.DataGrabber(infields=["subject_id"],
                                     outfields=outfields,
                                     base_directory=data_dir,
                                     template="%s/%s/%s"),
                     name="reggrabber")
reggrabber.inputs.template_args = dict(fa_image=[["subject_id", "dwi", "fa.nii"]],
                                       lowb_image=[["subject_id", "dwi", "lowb.nii"]],
                                       radial_diff=[["subject_id", "dwi", "radialdiff.nii"]],
                                       warpfield=[["subject_id", "normalization", "warpfield.nii.gz"]])
if args.r1:
    # Add in the R1 (actualy T1 at this stage) image if it was requested
    reggrabber.inputs.field_template = dict(t1_image=pjoin(
        project_dir,"Analysis/Nipype/flash/%s/tissue_parameters/T1.mgz"))
    reggrabber.inputs.template_args["t1_image"] = [["subject_id"]]

# Use BET to strip the lowb image and generate a mask
skullstrip = pe.Node(fsl.BET(frac=0.4, mask=True),
                     name="skullstrip")

# Erode the mask a bit (we only care about white matter after all)
erodemask = pe.Node(fsl.ImageMaths(op_string="-ero",
                                   suffix="_erode"),
                    name="erodemask")

# Use the brain mask to mask the FA image
maskfa = pe.Node(fsl.ImageMaths(op_string="-mas", suffix="_brain"), 
                 name="maskfa")

# And then mask the radialdiff image
maskrdiff = pe.Node(fsl.ImageMaths(op_string="-mas", suffix="_brain"),
                    name="maskrdiff")

# Use bbregister to align the lowb image to the Freesurfer structural
bbregister = pe.Node(fs.BBRegister(init="fsl",
                                   contrast_type="t2",
                                   epi_mask=True,
                                   out_fsl_file=True),
                     name="bbregister")

# Get a reference to the warp target (upsampled MNI brain)
warp_target =fsl.Info.standard_image("MNI152_T1_1mm.nii.gz")

# Apply the warpfield to the masked FA image using the bbregister affine matrix
warpfa = pe.Node(fsl.ApplyWarp(interp="spline",
                               ref_file=warp_target),
                    name="warpfa")

# And then warp the radialdiff image
warprdiff = pe.Node(fsl.ApplyWarp(interp="spline",
                                 ref_file=warp_target),
                    name="warprdiff")
                        

# If we're also using R1 images, process them here
if args.r1:
    convertt1 = pe.Node(fs.MRIConvert(out_type="niigz"),
                        name="convertt1")

    reciprocate = pe.Node(fsl.ImageMaths(op_string="-recip",
                                         suffix="_recip"),
                          name="reciprocate")

    warpr1 = pe.Node(fsl.ApplyWarp(interp="spline",
                                   ref_file=warp_target),
                     name="warpr1")

# Set up the data sink node
regsink = pe.Node(nio.DataSink(base_directory=analysis_dir,
                               parameterization=False,
                               substitutions=[("_brain_warp","")]),
                  name="regsink")

# Define the registration workflow
reg = pe.Workflow(name="registration", base_dir=working_dir)
archive_crashdumps(reg)

# Connect up the registration workflow
reg.connect([
    (subjsource,   reggrabber,  [("subject_id", "subject_id")]),
    (reggrabber,   skullstrip,  [("lowb_image", "in_file")]),
    (skullstrip,   erodemask,   [("mask_file", "in_file")]),
    (reggrabber,   maskfa,      [("fa_image", "in_file")]),
    (erodemask,    maskfa,      [("out_file", "in_file2")]),
    (reggrabber,   maskrdiff,   [("radial_diff", "in_file")]),
    (erodemask,    maskrdiff,   [("out_file", "in_file2")]),
    (subjsource,   bbregister,  [("subject_id", "subject_id")]),
    (skullstrip,   bbregister,  [("out_file", "source_file")]),
    (reggrabber,   warpfa,      [("warpfield", "field_file")]),
    (bbregister,   warpfa,      [("out_fsl_file", "premat")]),
    (maskfa,       warpfa,      [("out_file", "in_file")]),
    (reggrabber,   warprdiff,   [("warpfield", "field_file")]),
    (bbregister,   warprdiff,   [("out_fsl_file", "premat")]),
    (maskrdiff,    warprdiff,   [("out_file", "in_file")]),
    (reggrabber,   warprdiff,   [("warpfield", "field_file")]),
    (subjsource,   regsink,     [("subject_id", "container")]),
    (warpfa,       regsink,     [("out_file", "@fa_warped")]),
    (warprdiff,    regsink,     [("out_file", "@rdiff_warped")]),
    ])

if args.r1:
    regsink.inputs.substitutions.append(("T1_out_recip_warp", "R1"))
    reg.connect([
        (reggrabber,   convertt1,   [("t1_image", "in_file")]),
        (convertt1,    reciprocate, [("out_file", "in_file")]),
        (reggrabber,   warpr1,      [("warpfield", "field_file")]),
        (reciprocate,  warpr1,      [("out_file", "in_file")]),
        (warpr1,       regsink,     [("out_file", "@r1_warped")])
        ])

#---------------------------------------------------------------------#
# Skeletonise
#---------------------------------------------------------------------#

# Define the skeleton thresh 
# (it gets used a couple of times)
skeleton_thresh = 0.2

# Define outfields, optionally using R1 images
outfields=["fa_images"]
if args.r1:
    outfields.append("r1_images")

# Grab all of the normalized single-subject FA images in a sorted list
skelgrabber = pe.MapNode(nio.DataGrabber(infields=["subject_id"],
                                      outfields=outfields,
                                      base_directory=analysis_dir,
                                      sort_filelist=True,
                                      template="%s/%s.nii.gz"),
                       iterfield=["subject_id"],
                       name="skelgrabber")
skelgrabber.inputs.template_args = dict(fa_images=[["subject_id", "fa"]])
if args.r1:
    skelgrabber.inputs.template_args["r1_images"] = [["subject_id", "R1"]]
# This seems suboptimal but it appears not to cascade
skelgrabber.overwrite=True
skelgrabber.inputs.subject_id = subjects

# Merge the FA files into a 4D file
mergefa = pe.Node(fsl.Merge(dimension="t"), name="mergefa")

# Take the mean over the fourth dimension
meanfa = pe.Node(fsl.ImageMaths(op_string="-Tmean",
                                 suffix="_mean"),
                  name="meanfa")

# Use the mean FA volume to generate a tract skeleton
makeskeleton = pe.Node(fsl.TractSkeleton(skeleton_file=True),
                       name="makeskeleton")

# Mask the skeleton at the threshold
skeletonmask = pe.Node(fsl.ImageMaths(op_string="-thr %.1f -bin"%skeleton_thresh,
                                      suffix="_mask"),
                       name="skeletonmask")

# As we're in MNI space, just use the standard brain mask
brainmask = fsl.Info.standard_image("MNI152_T1_1mm_brain_mask.nii.gz")

# Invert the brainmask then add in the tract skeleton
invertmask = pe.Node(fsl.ImageMaths(in_file=brainmask,
                                    suffix="_inv",
                                    op_string="-mul -1 -add 1 -add"),
                     name="invertmask")

# Generate a distance map with the tract skeleton
distancemap = pe.Node(fsl.DistanceMap(),
                      name="distancemap")

# Project the FA values onto the skeleton
projectfa = pe.Node(fsl.TractSkeleton(threshold=skeleton_thresh,
                                      use_cingulum_mask=True,
                                      project_data=True),
                    name="projectfa")

# Process the R1 Files
if args.r1:
    
    # Merge
    merger1 = pe.Node(fsl.Merge(dimension="t"), name="merger1")

    # Project
    projectr1 = pe.Node(fsl.TractSkeleton(threshold=skeleton_thresh,
                                          use_cingulum_mask=True,
                                          project_data=True),
                        name="projectr1")
                      

# Define a dataink node for the skeleton workflow
skeletonsink = pe.Node(nio.DataSink(base_directory=pjoin(analysis_dir, "group"),
                                    parameterization=False,
                                    substitutions= [
     ("fa_merged_mean_skeleton_mask", "skeleton_mask"),
     ("fa_merged_mean_skeleton", "skeleton"),
     ("fa_merged_mean", "mean_fa"),
     ("fa_merged_skeletonised", "skeletonised_fa")]),
                       name="skeletonsink")
if args.r1:
    skeletonsink.inputs.substitutions.append(
        ("R1_merged_skeletonised", "skeletonised_R1"))

# Define the skeleton workflow
skeletor = pe.Workflow(name="skeletonise", base_dir=working_dir)
archive_crashdumps(skeletor)


def check_subjects(filelist):
    """Control the subjects used in the group stage"""
    if args.groupfile or args.clgroup:
        if args.groupfile:
            group = open(args.groupfile).read().strip().split("\n")
        else:
            group = args.subjects
        newfilelist = []
        for f in filelist:
            subj = f.split(os.path.sep)[-2]
            if subj in group:
                newfilelist.append(f)
        return newfilelist
    return filelist

# And connect it up
skeletor.connect([
    (skelgrabber,     mergefa,         [("fa_images", "in_files")]),
    (mergefa,         meanfa,          [("merged_file", "in_file")]),
    (meanfa,          makeskeleton,    [("out_file", "in_file")]),
    (makeskeleton,    skeletonmask,    [("skeleton_file", "in_file")]),
    (skeletonmask,    invertmask,      [("out_file", "in_file2")]),
    (invertmask,      distancemap,     [("out_file", "in_file")]),
    (distancemap,     projectfa,       [("distance_map", "distance_map")]),
    (meanfa,          projectfa,       [("out_file", "in_file")]),
    (mergefa,         projectfa,       [("merged_file", "data_file")]),
    (meanfa,          skeletonsink,    [("out_file", "@mean_fa")]),
    (projectfa,       skeletonsink,    [("projected_data", "@projected_fa")]),
    (makeskeleton,    skeletonsink,    [("skeleton_file", "@skeleton")]),
    (skeletonmask,    skeletonsink,    [("out_file", "@skeleton_mask")]),
    ])

if args.r1:
    skeletor.connect([
        (skelgrabber,     merger1,         [("r1_images", "in_files")]),
        (merger1,         projectr1,       [("merged_file", "alt_data_file")]),
        (meanfa,          projectr1,       [("out_file", "in_file")]),
        (mergefa,         projectr1,       [("merged_file", "data_file")]),
        (distancemap,     projectr1,       [("distance_map", "distance_map")]),
        (projectr1,       skeletonsink,    [("projected_data", "@projected_r1")]),
        ])

if args.ipython:
    plugin = "IPython"
elif args.multiproc:
    plugin="MultiProc"

def workflow_runner(flow, stem):
    if any([a.startswith(stem) for a in args.workflows]) or args.workflows==["all"]:
        flow.run(plugin=plugin)

if __name__ == "__main__":
    # Run some things
    workflow_runner(reg, "reg")
    workflow_runner(skeletor, "skel")
