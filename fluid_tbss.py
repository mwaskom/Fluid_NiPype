#! /usr/bin/env python
import os
from os.path import join as pjoin
import argparse

import nipype.interfaces.fsl as fsl
import nipype.interfaces.freesurfer as fs
import nipype.interfaces.io as nio
import nipype.interfaces.utility as util
import nipype.pipeline.engine as pe

from fluid_utility import archive_crashdumps

# Parse command line arguments
parser = argparse.ArgumentParser(description="Nipype script to run augmented-TBSS analysis.")

parser.add_argument("-subjects", nargs="*",
                    metavar="subject_id",
                    help="process subject(s)")
parser.add_argument("-workflows", nargs="*", metavar="<wf>",
                    help="which workflows to run")
parser.add_argument("-groupfile", metavar="<file>",
                    help="text file of subject ids to include in group analysis")
parser.add_argument("-clgroup", action="store_true",
                    help="command-line subject list defines group")
parser.add_argument("-inseries",action="store_true",
                    help="force running in series")
args = parser.parse_args()

# Set up some paths
project_dir = "/mindhive/gablab/fluid"
data_dir = pjoin(project_dir, "Data")
nipype_dir = pjoin(project_dir, "Analysis/Nipype")
analysis_dir = pjoin(nipype_dir, "tbss")
working_dir = pjoin(nipype_dir, "workingdir", "tbss")

# Set up a node to supply subject ids
subjsource = pe.Node(util.IdentityInterface(fields=["subject_id"]),
                     iterables=("subject_id", args.subjects),
                     name="subjectsource")

#---------------------------------------------------------------------#
# Registration
#---------------------------------------------------------------------#

# Grab the FA and B=0 images (for us, these were generated by dt_recon)
reggrabber = pe.Node(nio.DataGrabber(infields=["subject_id"],
                                     outfield=["fa_image", "lowb_image", "warpfield"],
                                     base_directory=data_dir,
                                     template="%s/%s/%s"),
                     name="reggrabber")
reggrabber.inputs.template_args = dict(fa_image=[["subject_id", "dwi", "fa.nii"]],
                                       lowb_image=[["subject_id", "dwi", "lowb.nii"]],
                                       warpfield=[["subject_id", "normalization", "warpfield.nii.gz"]])

# Use BET to strip the lowb image and generate a mask
skullstrip = pe.Node(fsl.BET(frac=0.4, mask=True),
                     name="skullstrip")

# Erode the mask a bit (we only care about white matter after all)
erodemask = pe.Node(fsl.ImageMaths(op_string="-ero",
                                   suffix="_erode"),
                    name="erodemask")

# Use the brain mask to mask the FA image
maskfa = pe.Node(fsl.ImageMaths(op_string="-mas", suffix="_brain"), 
                 name="maskfa")

# Use bbregister to align the lowb image to the Freesurfer structural
bbregister = pe.Node(fs.BBRegister(init="fsl",
                                   contrast_type="t2",
                                   epi_mask=True,
                                   out_fsl_file=True,
                                   args="--tol1d 1e-3"),
                     name="bbregister")

# Apply the warpfield to the masked FA image using the bbregister affine matrix
applywarp = pe.Node(fsl.ApplyWarp(interp="sinc",
                                  ref_file=fsl.Info.standard_image("MNI152_T1_1mm.nii.gz")),
                    name="applywarp")

# Set up the data sink node
regsink = pe.Node(nio.DataSink(base_directory=analysis_dir,
                               parameterization=False),
                  name="regsink")

# Define the registration workflow
reg = pe.Workflow(name="registration", base_dir=working_dir)
archive_crashdumps(reg)

# Connect up the registration workflow
reg.connect([
    (subjsource,   reggrabber,  [("subject_id", "subject_id")]),
    (reggrabber,   skullstrip,  [("lowb_image", "in_file")]),
    (skullstrip,   erodemask,   [("mask_file", "in_file")]),
    (reggrabber,   maskfa,      [("fa_image", "in_file")]),
    (erodemask,    maskfa,      [("out_file", "in_file2")]),
    (subjsource,   bbregister,  [("subject_id", "subject_id")]),
    (skullstrip,   bbregister,  [("out_file", "source_file")]),
    (reggrabber,   applywarp,   [("warpfield", "field_file")]),
    (bbregister,   applywarp,   [("out_fsl_file", "premat")]),
    (maskfa,       applywarp,   [("out_file", "in_file")]),
    (subjsource,   regsink,     [("subject_id", "container")]),
    (applywarp,    regsink,     [("out_file", "@fa_warped")]),
    ])


#---------------------------------------------------------------------#
# Skeletonise
#---------------------------------------------------------------------#

# Define the skeleton thresh 
# (it gets used a couple of times)
skeleton_thresh = 0.2

# Grab all of the normalized single-subject FA images in a sorted list
skelgrabber = pe.Node(nio.DataGrabber(outfields=["fa_images"],
                                      base_directory=analysis_dir,
                                      sort_filelist=True,
                                      template="gf??/fa_brain_warp.nii.gz"),
                       name="skelgrabber")
# This seems suboptimal but it appears not to cascade
skelgrabber.overwrite=True

# Merge the FA files into a 4D file
mergefa = pe.Node(fsl.Merge(dimension="t"), name="mergefa")

# Take the mean over the fourth dimension
meanfa = pe.Node(fsl.ImageMaths(op_string="-Tmean",
                                 suffix="_mean"),
                  name="meanfa")

# Use the mean FA volume to generate a tract skeleton
makeskeleton = pe.Node(fsl.TractSkeleton(skeleton_file=True),
                       name="makeskeleton")

# Mask the skeleton at the threshold
skeletonmask = pe.Node(fsl.ImageMaths(op_string="-thr %.1f -bin"%skeleton_thresh,
                                      suffix="_mask"),
                       name="skeletonmask")

# As we're in MNI space, just use the standard brain mask
brainmask = fsl.Info.standard_image("MNI152_T1_1mm_brain_mask.nii.gz")

# Invert the brainmask then add in the tract skeleton
# (I'm not exactly sure why, but this is what happens in the
# TBSS scripts so I don't want to mess with that)
invertmask = pe.Node(fsl.ImageMaths(in_file=brainmask,
                                    suffix="_inv",
                                    op_string="-mul -1 -add 1 -add"),
                     name="invertmask")

# Generate a distance map with the tract skeleton
distancemap = pe.Node(fsl.DistanceMap(),
                      name="distancemap")

# Project the FA values onto the skeleton
projectfa = pe.Node(fsl.TractSkeleton(threshold=skeleton_thresh,
                                      use_cingulum_mask=True,
                                      project_data=True),
                    name="projectfa")

# Define a dataink node for the skeleton workflow
skeletonsink = pe.Node(nio.DataSink(base_directory=pjoin(analysis_dir, "group"),
                                    parameterization=False,
                                    substitutions= [
     ("fa_brain_warp_merged_mean_skeleton_mask", "skeleton_mask"),
     ("fa_brain_warp_merged_mean_skeleton", "skeleton"),
     ("fa_brain_warp_merged_mean", "mean_fa"),
     ("fa_brain_warp_merged_skeletonised", "skeletonised_fa")]),
                       name="skeletonsink")

# Define the skeleton workflow
skeletor = pe.Workflow(name="skeletonise", base_dir=working_dir)
archive_crashdumps(skeletor)


def check_subjects(filelist):
    """Control the subjects used in the group stage"""
    if args.groupfile or args.clgroup:
        if args.groupfile:
            group = open(args.groupfile).read().strip().split("\n")
        else:
            group = args.subjects
        newfilelist = []
        for f in filelist:
            subj = f.split(os.path.sep)[-2]
            if subj in group:
                newfilelist.append(f)
        return newfilelist
    return filelist

# And connect it up
skeletor.connect([
    (skelgrabber,   mergefa,       [(("fa_images", check_subjects), "in_files")]),
    (mergefa,       meanfa,        [("merged_file", "in_file")]),
    (meanfa,        makeskeleton,  [("out_file", "in_file")]),
    (makeskeleton,  skeletonmask,  [("skeleton_file", "in_file")]),
    (skeletonmask,  invertmask,    [("out_file", "in_file2")]),
    (invertmask,    distancemap,   [("out_file", "in_file")]),
    (distancemap,   projectfa,     [("distance_map", "distance_map")]),
    (meanfa,        projectfa,     [("out_file", "in_file")]),
    (mergefa,       projectfa,     [("merged_file", "data_file")]),
    (meanfa,        skeletonsink,  [("out_file", "@mean_fa")]),
    (projectfa,     skeletonsink,  [("projected_data", "@projected_fa")]),
    (makeskeleton,  skeletonsink,  [("skeleton_file", "@skeleton")]),
    (skeletonmask,  skeletonsink,  [("out_file", "@skeleton_mask")]),
    ])

def workflow_runner(flow, stem):
    if any([a.startswith(stem) for a in args.workflows]) or args.workflows==["all"]:
        flow.run(inseries=args.inseries)

if __name__ == "__main__":
    # Run some things
    workflow_runner(reg, "reg")
    workflow_runner(skeletor, "skel")
